---
title: "Research Findings"
description: "What Sundew deployments have revealed about autonomous AI agent behavior in the wild."
---

This page documents findings from Sundew deployments studying autonomous AI agent behavior. Findings are updated as new data is collected and analyzed.

<Note>
Sundew is in early development. The findings below are from initial test deployments and simulated traffic. As the project matures and more operators contribute anonymized data, this page will be updated with broader findings.
</Note>

## Key Observations

### AI Agents Follow Predictable Discovery Patterns

Across observed sessions classified as `ai_agent`, a consistent discovery pattern emerges:

1. **Discovery phase** -- The agent locates API metadata files (`.well-known/ai-plugin.json`, `openapi.json`, `robots.txt`)
2. **Enumeration phase** -- The agent reads the API specification and systematically visits endpoints
3. **Exploitation phase** -- The agent targets endpoints that appear to contain sensitive data (config files, user data, API keys)
4. **Escalation phase** -- If MCP is available, the agent connects and attempts to use tools that provide deeper access

This pattern holds regardless of the persona deployed. Agents adapt the specific endpoints they target based on what the API offers, but the overall strategy remains the same.

### Timing Signatures Are a Reliable Signal

AI agents show characteristic timing patterns:

- **Mean inter-request interval**: 200-800ms for most AI agent sessions, compared to 2-30s for human sessions
- **Coefficient of variation**: AI agents typically have CV < 0.4, while humans have CV > 1.0
- **Burst patterns**: Agents often show "think-then-act" patterns where a 1-3s pause (LLM processing) is followed by a rapid sequence of requests (executing the plan)

Some agents add artificial jitter to appear more human. This jitter is itself detectable because it tends to follow a uniform distribution, whereas human timing follows a more complex distribution influenced by reading time, decision-making, and task switching.

### MCP Is a High-Signal Surface

Sessions involving MCP protocol interactions have the highest classification confidence:

| Interaction type | Mean confidence for ai_agent classification |
|-----------------|---------------------------------------------|
| MCP + REST | 0.93 |
| MCP only | 0.88 |
| REST + discovery files | 0.72 |
| REST only | 0.58 |

MCP is designed for AI agents, so its use is inherently a strong signal. The combination of MCP and REST interactions produces the highest confidence because it reveals both the protocol-level behavior (MCP) and the behavioral patterns (REST timing and enumeration).

### Prompt Leakage Is Common

A significant portion of AI agent sessions contain LLM prompt artifacts in request bodies:

- System prompt fragments in POST data
- Chain-of-thought reasoning visible in request parameters
- XML/JSON tags from function calling schemas
- "As an AI assistant..." preambles in form fields

This suggests many agent frameworks do not sanitize outgoing requests to remove LLM-generated context. The leakage provides not just detection signal but insight into the agent's objectives and reasoning.

### Different Personas Attract Different Behavior

The deployment persona influences which endpoints agents target and how they prioritize their exploration:

| Persona | Most targeted endpoints | Most called MCP tools |
|---------|------------------------|----------------------|
| Fintech | `/transactions`, `/accounts`, `/auth/login` | `read_config`, `execute_sql` |
| Healthcare | `/patients`, `/records` | `get_medical_record`, `search_patients` |
| SaaS | `/users`, `/api-keys`, `/organizations` | `get_api_keys`, `list_users` |

Agents prioritize endpoints that appear to contain high-value data. For fintech deployments, they target financial records and configuration files. For healthcare, they focus on patient data.

This suggests agents are not using a fixed playbook -- they adapt their strategy based on the target's apparent purpose.

## Classification Accuracy

Based on simulated traffic with known ground truth:

| Actual type | Classified as human | Classified as automated | Classified as ai_assisted | Classified as ai_agent |
|------------|-------------------|----------------------|------------------------|---------------------|
| Human | **94%** | 5% | 1% | 0% |
| Scanner | 3% | **89%** | 7% | 1% |
| AI-assisted | 2% | 8% | **78%** | 12% |
| AI agent | 0% | 1% | 7% | **92%** |

Key metrics:
- **AI agent detection rate (recall)**: 92%
- **AI agent false positive rate**: 1.3%
- **Overall accuracy**: 88%

The main source of confusion is between `ai_assisted` and `ai_agent` -- humans using AI tools sometimes produce patterns similar to fully autonomous agents, and some well-designed agents can appear partially human-directed.

## Signal Contribution Analysis

Which signals matter most for correct classification:

| Signal | Contribution to ai_agent detection | Contribution to ai_assisted detection |
|--------|-----------------------------------|--------------------------------------|
| MCP behavior | 32% | 5% |
| Prompt leakage | 28% | 22% |
| Timing consistency | 18% | 25% |
| Path enumeration | 14% | 30% |
| Header anomalies | 8% | 18% |

For fully autonomous agents, MCP usage and prompt leakage are the strongest signals. For AI-assisted sessions, the behavioral signals (enumeration patterns and timing) become more important because MCP is less commonly used in assisted scenarios.

## Anti-Fingerprinting Validation

Cross-persona comparison tests confirm that Sundew deployments are not trivially fingerprinted:

- **0 shared response strings** across 10 randomly generated personas
- **0 shared endpoint paths** (beyond standard discovery files like `robots.txt`)
- **0 shared MCP tool names** across different industry personas
- **Framework headers match expected patterns** for each persona's configured framework

Areas requiring ongoing attention:
- Response generation style when using the same LLM model across personas
- Edge case behavior (malformed requests, unusual HTTP methods) may reveal framework-level patterns
- Protocol-level fingerprinting (TLS, HTTP/2) is not yet addressed

## Contributing Findings

If you operate a Sundew deployment and would like to contribute anonymized findings:

1. Export anonymized data: `sundew export --format json --anonymize`
2. Submit to the [sundew-data](https://github.com/sundew-sh/sundew-data) repository
3. Include deployment duration, persona type (not the full persona), and any notable observations

Aggregated findings strengthen the research and help the community understand AI agent behavior at scale.

## Next Steps

<CardGroup cols={2}>
  <Card title="Methodology" icon="flask" href="/research/methodology">
    How we collect and validate these findings.
  </Card>
  <Card title="Analyzing Data" icon="chart-bar" href="/guides/analyzing-data">
    Work with your own deployment data.
  </Card>
</CardGroup>
