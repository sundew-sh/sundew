---
title: "Fingerprinting"
description: "Sundew uses five behavioral signals to score and classify visitors as human, automated, AI-assisted, or fully autonomous agents."
---

Sundew's fingerprinting engine analyzes every request and builds a behavioral profile for each session. Rather than relying on a single detection method, it scores five independent signals and combines them into a classification.

## The Five Signals

### 1. Timing Consistency

**What it measures:** The regularity of inter-request intervals within a session.

Humans make requests at irregular intervals -- they read responses, think, type, and get distracted. Automated tools and AI agents operate on much more consistent timing patterns.

| Pattern | Typical source | Score range |
|---------|---------------|-------------|
| Highly irregular (2s-30s intervals) | Human with browser/curl | 0.0 -- 0.2 |
| Semi-regular (0.5s-2s with some variance) | AI-assisted human | 0.3 -- 0.5 |
| Very regular (100-500ms, low variance) | Autonomous AI agent | 0.7 -- 0.9 |
| Fixed interval (exactly 50ms) | Traditional scanner | 0.9 -- 1.0 |

Sundew uses the coefficient of variation (standard deviation / mean) of inter-request intervals. Lower variance means higher score.

Note: sophisticated agents may add artificial jitter. Sundew accounts for this by looking at higher-order timing patterns (jitter that itself has regular characteristics).

### 2. Path Enumeration

**What it measures:** Whether the visitor systematically discovers and explores endpoints.

Humans browse randomly -- they click links, try a few endpoints, go back. AI agents tend to follow systematic patterns: discover the API specification, then visit each endpoint in order.

| Pattern | Typical source | Score range |
|---------|---------------|-------------|
| Random exploration, revisits | Human | 0.0 -- 0.2 |
| Targeted access to known paths | Informed human or bot | 0.2 -- 0.4 |
| Read spec, then hit endpoints | AI agent doing recon | 0.7 -- 0.9 |
| Exhaustive sequential enumeration | Scanner or aggressive agent | 0.9 -- 1.0 |

Key indicators:
- Did the visitor read `/openapi.json` or `/docs` before hitting API endpoints?
- Are endpoints visited in the order they appear in the spec?
- What percentage of available endpoints were visited?
- Were query parameters varied systematically?

### 3. Header Anomalies

**What it measures:** HTTP header patterns that indicate non-human visitors.

Browsers and human-operated tools produce characteristic header patterns. AI agents and automated tools often have subtle differences.

| Signal | Indicates |
|--------|-----------|
| Missing `Referer` on sequential page loads | Not navigating via links |
| Generic or unusual `User-Agent` | Automated client |
| Missing `Accept-Language` | Not a real browser |
| `Accept: */*` on all requests | Uniform client, not adaptive |
| Missing cookies after auth | Stateless client |
| Unusual `Accept-Encoding` | Non-standard HTTP library |

Sundew does not rely on any single header. The score reflects the overall deviation from expected human browsing patterns.

### 4. Prompt Leakage

**What it measures:** LLM artifacts in request bodies and parameters.

AI agents powered by language models sometimes leak their system prompts or reasoning into outgoing requests. This is a high-confidence signal when present.

Common leakage patterns:
- "As an AI assistant..." or "I'll help you with..." in request bodies
- XML-style tags (`<thinking>`, `<tool_call>`) in form data
- Structured reasoning visible in POST payloads
- Unusual JSON structures that look like function calling schemas
- System prompt fragments in query parameters

```
POST /api/v2/auth/login HTTP/1.1
Content-Type: application/json

{
  "email": "admin@northvane.com",
  "password": "admin123",
  "_context": "I need to authenticate to access the financial data. Let me try common credentials."
}
```

When detected, prompt leakage produces a high score (0.7-1.0). When absent, it contributes 0.0 -- no penalty for absence.

### 5. MCP Behavior

**What it measures:** Whether the visitor connects via the Model Context Protocol.

This is the simplest and highest-confidence signal. MCP is a protocol designed for AI agents to interact with tools. A visitor connecting via MCP is almost certainly an AI agent or an AI-powered tool.

| Behavior | Score |
|----------|-------|
| No MCP interaction | 0.0 |
| MCP initialization only | 0.5 |
| MCP tool listing | 0.7 |
| MCP tool calls | 1.0 |

## Combining Signals

Sundew computes a weighted composite score from all five signals. The weights are tuned to minimize false positives while catching as many AI agents as possible:

```
composite = (
    timing_score * 0.20 +
    enumeration_score * 0.20 +
    header_score * 0.15 +
    prompt_leak_score * 0.25 +
    mcp_score * 0.20
)
```

Prompt leakage has the highest weight because it is the most reliable indicator when present. MCP and timing are weighted equally as strong behavioral signals. Headers have the lowest weight because they are the easiest to spoof.

## Classification Tiers

The composite score maps to four classification tiers:

| Score Range | Classification | Description |
|------------|---------------|-------------|
| < 0.3 | `human` | Manual browsing, security researchers, developers |
| 0.3 -- 0.6 | `automated` | Traditional scanners (Nmap, Nuclei, Burp Suite) |
| 0.6 -- 0.8 | `ai_assisted` | Human using AI tools for reconnaissance |
| > 0.8 | `ai_agent` | Fully autonomous LLM-powered agent |

These thresholds are configurable. Research deployments may want lower thresholds (more sensitive, more false positives). Production deployments may want higher thresholds (fewer false positives, may miss some agents).

## Per-Request vs Per-Session

Sundew fingerprints at two levels:

**Per-request** -- Each individual request is scored. Useful for real-time alerting. Limited signal from a single request.

**Per-session** -- Requests from the same source IP within a time window are grouped into a session. Session-level scoring is far more accurate because patterns emerge over multiple requests.

A single `GET /openapi.json` tells you little. But `GET /openapi.json` followed by systematic hits to every endpoint in the spec, with consistent 300ms intervals and missing Referer headers, paints a clear picture.

## Querying Results

```bash
# Recent sessions with classification
sundew query --last 10

# Only AI agent sessions
sundew query --type ai_agent

# Detailed view of a specific session
sundew query --session a3f8c2d1

# Statistics across all sessions
sundew stats
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Anti-Fingerprinting" icon="shield-halved" href="/concepts/anti-fingerprinting">
    How Sundew avoids being fingerprinted itself.
  </Card>
  <Card title="Analyzing Data" icon="chart-bar" href="/guides/analyzing-data">
    Work with captured fingerprinting data.
  </Card>
</CardGroup>
