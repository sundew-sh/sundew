---
title: "Anti-Fingerprinting"
description: "How Sundew prevents AI agents from detecting that they are interacting with a honeypot."
---

Anti-fingerprinting is Sundew's existential challenge. If AI agents can learn to detect Sundew, the project fails. This page explains the problem, Sundew's approach, and the ongoing validation process.

## The Problem

Traditional honeypots have a known weakness: they produce detectable patterns. Security researchers maintain databases of honeypot signatures -- specific response strings, default configurations, behavioral quirks. Tools like Shodan and Honeyscore actively identify honeypots on the internet.

AI agents make this problem worse. An autonomous agent can:

1. Search the web for "sundew honeypot" and read the source code
2. Build a mental model of what Sundew deployments look like
3. Test every service it encounters against those patterns
4. Avoid anything that matches, or modify its behavior to appear human

This is fundamentally different from human attackers, who rarely have comprehensive honeypot signature databases loaded in memory during an engagement.

## Sundew's Approach

Sundew's defense is the persona engine. Instead of having fixed, learnable signatures, every deployment generates a unique identity. The key insight: **knowing what Sundew is provides zero advantage in detecting any specific deployment.**

### What Varies Between Deployments

| Aspect | How it varies |
|--------|--------------|
| Company name and branding | Unique per persona |
| Endpoint paths | `/api/v2/transactions` vs `/v1/users` vs `/rest/patients` |
| Response body structure | Different field names, nesting, formats |
| HTTP headers | Different Server, X-Powered-By, framework headers |
| Error messages | Different wording, format, verbosity |
| Timing profile | Different latency ranges per persona |
| MCP tool names | `query_transactions` vs `list_users` vs `search_patients` |
| API documentation | Different spec structure, descriptions |
| Discovery files | Different manifests, sitemaps, robots.txt |
| Data content | Financial records vs medical records vs user accounts |
| Auth scheme | JWT vs API key vs OAuth2 vs Basic |

### What Must Never Leak

These are the invariants Sundew enforces:

1. **No shared response strings** -- No two personas produce the same response body for any endpoint
2. **No structural tells** -- Response JSON structures vary (array vs object wrapping, pagination style, ID formats)
3. **No code artifacts** -- Error messages, stack traces, and debug info are persona-generated, not from Sundew's actual code
4. **No timing signatures** -- Latency jitter ranges differ per persona, and no deployment uses Sundew's processing time as a signal
5. **No protocol artifacts** -- MCP implementation details do not leak Sundew-specific patterns
6. **No dependency fingerprints** -- Sundew's actual tech stack (FastAPI, Python) is hidden behind the persona's framework fingerprint

## Validation

Anti-fingerprinting is validated continuously through automated tests and periodic audits.

### Automated Tests

The test suite includes cross-persona comparison tests:

```
Test: anti_fingerprinting_validation
  1. Generate 10 random personas
  2. Start Sundew with each persona
  3. Collect all observable outputs (responses, headers, timing)
  4. Compare across all pairs of deployments
  5. FAIL if any two deployments share:
     - Response body content (beyond boilerplate HTTP)
     - Endpoint paths
     - Framework-identifying headers
     - Error message text
     - MCP tool names or descriptions
     - Timing profile characteristics
```

### The "Google Test"

Can someone who has read Sundew's source code detect a running deployment?

This is tested by:
1. Reading the full Sundew codebase
2. Examining a running Sundew deployment from the outside
3. Attempting to distinguish it from a real Express.js API, a real Django API, or a real Spring Boot API
4. Documenting any detectable patterns

If distinguishable patterns are found, they are treated as bugs and fixed.

### The "Corpus Test"

Does knowing what Persona A looks like help detect Persona B?

1. Fully interact with Deployment A (all endpoints, MCP, discovery)
2. Record everything observable
3. Attempt to identify Deployment B as Sundew using only knowledge from A
4. If any cross-persona patterns exist, fix them

## Known Challenges

### LLM Style Patterns

When Sundew uses an LLM to generate response templates, the LLM may produce stylistically similar outputs across personas. Mitigations:

- Different system prompts per persona with explicit style guidance
- Temperature variation to increase output diversity
- Post-generation validation to check for cross-persona similarity
- Support for multiple LLM backends (different models produce different styles)

### Persona Pack Reuse

Deployments using the same pre-built persona pack will share response patterns. This is a known limitation of the no-LLM mode. Mitigations:

- Ship multiple variants per industry theme
- Document this limitation clearly
- Recommend LLM-backed generation for research deployments

### Deep Protocol Analysis

Sophisticated agents may analyze subtle protocol-level patterns:
- TCP/TLS fingerprinting (mitigated by running behind standard reverse proxies)
- HTTP/2 frame ordering (mitigated by using standard ASGI servers)
- WebSocket upgrade behavior (future work)

### Behavioral Tells

Sundew's actual behavior in edge cases might differ from real services:
- How it handles malformed requests
- Rate limiting behavior
- Session management details
- WebSocket or SSE behavior (not yet implemented)

These are tracked as ongoing improvements in the roadmap.

## Threat Model

| Threat | Sundew's defense | Confidence |
|--------|-----------------|------------|
| Agent reads Sundew source code | Persona engine -- source code does not reveal deployment signatures | High |
| Agent has seen another Sundew deployment | Cross-persona variation -- no shared patterns | High |
| Agent uses known honeypot signature databases | No fixed signatures to catalog | High |
| Agent analyzes response generation style | LLM variation, temperature tuning, multiple models | Medium |
| Agent does deep protocol analysis | Standard infrastructure (nginx, ASGI) | Medium |
| Agent probes edge case behavior | Ongoing improvement area | Medium |

## Contributing to Anti-Fingerprinting

If you discover a way to fingerprint Sundew deployments, please report it. This is the project's most critical area. See [SECURITY.md](https://github.com/sundew-sh/sundew/blob/main/SECURITY.md) for responsible disclosure.

Contributions that improve anti-fingerprinting -- new validation tests, edge case fixes, protocol-level mitigations -- are especially welcome.

## Next Steps

<CardGroup cols={2}>
  <Card title="Personas" icon="masks-theater" href="/concepts/personas">
    How the persona engine generates unique identities.
  </Card>
  <Card title="Research Methodology" icon="flask" href="/research/methodology">
    How we validate Sundew's effectiveness.
  </Card>
</CardGroup>
